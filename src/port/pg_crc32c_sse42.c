/*-------------------------------------------------------------------------
 *
 * pg_crc32c_sse42.c
 *	  Compute CRC-32C checksum using Intel SSE 4.2 instructions.
 *
 * 	  For longer inputs, we use carryless multiplication on SIMD registers,
 *	  based on: "Fast CRC Computation for Generic Polynomials Using PCLMULQDQ
 *	  Instruction" V. Gopal, E. Ozturk, et al., 2009
 *
 * Portions Copyright (c) 1996-2025, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *
 * IDENTIFICATION
 *	  src/port/pg_crc32c_sse42.c
 *
 *-------------------------------------------------------------------------
 */
#include "c.h"

#include <nmmintrin.h>
#include <wmmintrin.h>

#include "port/pg_crc32c.h"

#if defined(USE_SSE42_CRC32C) || defined(USE_SSE42_CRC32C_WITH_RUNTIME_CHECK)

#define PCLMUL_THRESHOLD 128
#define CRC_CASE(n) do {crc = (uint32) _mm_crc32_u64(crc, *((const uint64 *) (p - (n)*sizeof(uint64_t))));} while(0)

pg_attribute_no_sanitize_alignment()
pg_attribute_target("sse4.2")
pg_crc32c
pg_comp_crc32c_sse42(pg_crc32c crc, const void *data, size_t len)
{
	const unsigned char *p = data;
	const unsigned char *pend = p + len;

	/*
	 * Process eight bytes of data at a time.
	 *
	 * NB: We do unaligned accesses here. The Intel architecture allows that,
	 * and performance testing didn't show any performance gain from aligning
	 * the begin address.
	 */
#ifdef __x86_64__

	/* set p to end of last word boundary */
	p = pend - len % (sizeof(uint64_t));
	Assert (len < PCLMUL_THRESHOLD);

	switch (len / sizeof(uint64_t))
	{
		case 15: CRC_CASE(15); /* FALLTHROUGH */
		case 14: CRC_CASE(14); /* FALLTHROUGH */
		case 13: CRC_CASE(13); /* FALLTHROUGH */
		case 12: CRC_CASE(12); /* FALLTHROUGH */
		case 11: CRC_CASE(11); /* FALLTHROUGH */
		case 10: CRC_CASE(10); /* FALLTHROUGH */
		case 9: CRC_CASE(9); /* FALLTHROUGH */
		case 8: CRC_CASE(8); /* FALLTHROUGH */
		case 7: CRC_CASE(7); /* FALLTHROUGH */
		case 6: CRC_CASE(6); /* FALLTHROUGH */
		case 5: CRC_CASE(5); /* FALLTHROUGH */
		case 4: CRC_CASE(4); /* FALLTHROUGH */
		case 3: CRC_CASE(3); /* FALLTHROUGH */
		case 2: CRC_CASE(2); /* FALLTHROUGH */
		case 1: CRC_CASE(1); /* FALLTHROUGH */
		case 0: break;
		default: pg_unreachable();
	}

	/* Process remaining full four bytes if any */
	if (p + 4 <= pend)
	{
		crc = _mm_crc32_u32(crc, *((const unsigned int *) p));
		p += 4;
	}
#else

	/*
	 * Process four bytes at a time. (The eight byte instruction is not
	 * available on the 32-bit x86 architecture).
	 */
	while (p + 4 <= pend)
	{
		crc = _mm_crc32_u32(crc, *((const unsigned int *) p));
		p += 4;
	}
#endif							/* __x86_64__ */

	/* Process any remaining bytes one at a time. */
	while (p < pend)
	{
		crc = _mm_crc32_u8(crc, *p);
		p++;
	}

	return crc;
}
#endif // USE_SSE42_CRC32C or USE_SSE42_CRC32C_WITH_RUNTIME_CHECK

#if defined(USE_PCLMUL_CRC32C) || defined(USE_PCLMUL_CRC32C_WITH_RUNTIME_CHECK)

/* Generated by https://github.com/corsix/fast-crc32/ using: */
/* ./generate -i sse -p crc32c -a v4 */
/* MIT licensed */

#define clmul_lo(a, b) (_mm_clmulepi64_si128((a), (b), 0))
#define clmul_hi(a, b) (_mm_clmulepi64_si128((a), (b), 17))

pg_attribute_target("sse4.2,pclmul")
pg_crc32c
pg_comp_crc32c_pclmul(pg_crc32c crc, const void *data, size_t length)
{
	/* adjust names to match generated code */
	pg_crc32c	crc0 = crc;
	size_t		len = length;
	const unsigned char *buf = data;

#if SIZEOF_VOID_P >= 8
	if (len >= PCLMUL_THRESHOLD)
	{
		/* First vector chunk. */
		__m128i		x0 = _mm_loadu_si128((const __m128i *) buf),
					y0;
		__m128i		x1 = _mm_loadu_si128((const __m128i *) (buf + 16)),
					y1;
		__m128i		x2 = _mm_loadu_si128((const __m128i *) (buf + 32)),
					y2;
		__m128i		x3 = _mm_loadu_si128((const __m128i *) (buf + 48)),
					y3;
		__m128i		k;

		k = _mm_setr_epi32(0x740eef02, 0, 0x9e4addf8, 0);
		x0 = _mm_xor_si128(_mm_cvtsi32_si128(crc0), x0);
		buf += 64;
		len -= 64;

		/* Main loop. */
		while (len >= 64)
		{
			y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
			y1 = clmul_lo(x1, k), x1 = clmul_hi(x1, k);
			y2 = clmul_lo(x2, k), x2 = clmul_hi(x2, k);
			y3 = clmul_lo(x3, k), x3 = clmul_hi(x3, k);
			y0 = _mm_xor_si128(y0, _mm_loadu_si128((const __m128i *) buf)), x0 = _mm_xor_si128(x0, y0);
			y1 = _mm_xor_si128(y1, _mm_loadu_si128((const __m128i *) (buf + 16))), x1 = _mm_xor_si128(x1, y1);
			y2 = _mm_xor_si128(y2, _mm_loadu_si128((const __m128i *) (buf + 32))), x2 = _mm_xor_si128(x2, y2);
			y3 = _mm_xor_si128(y3, _mm_loadu_si128((const __m128i *) (buf + 48))), x3 = _mm_xor_si128(x3, y3);
			buf += 64;
			len -= 64;
		}

		/* Reduce x0 ... x3 to just x0. */
		k = _mm_setr_epi32(0xf20c0dfe, 0, 0x493c7d27, 0);
		y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
		y2 = clmul_lo(x2, k), x2 = clmul_hi(x2, k);
		y0 = _mm_xor_si128(y0, x1), x0 = _mm_xor_si128(x0, y0);
		y2 = _mm_xor_si128(y2, x3), x2 = _mm_xor_si128(x2, y2);
		k = _mm_setr_epi32(0x3da6d0cb, 0, 0xba4fc28e, 0);
		y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
		y0 = _mm_xor_si128(y0, x2), x0 = _mm_xor_si128(x0, y0);

		/* Reduce 128 bits to 32 bits, and multiply by x^32. */
		crc0 = _mm_crc32_u64(0, _mm_extract_epi64(x0, 0));
		crc0 = _mm_crc32_u64(crc0, _mm_extract_epi64(x0, 1));
	}
#endif /* SIZEOF_VOID_P */

	return pg_comp_crc32c_sse42(crc0, buf, len);
}
#endif // USE_PCLMUL_CRC32C or USE_PCLMUL_CRC32C_WITH_RUNTIME_CHECK
