/*-------------------------------------------------------------------------
 *
 * pg_crc32c_sse42.c
 *	  Compute CRC-32C checksum using Intel SSE 4.2 instructions.
 *
 * Portions Copyright (c) 1996-2025, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *
 * IDENTIFICATION
 *	  src/port/pg_crc32c_sse42.c
 *
 *-------------------------------------------------------------------------
 */
#include "c.h"

#include <nmmintrin.h>
#include <wmmintrin.h>

#include "port/pg_crc32c.h"

pg_attribute_no_sanitize_alignment()
pg_attribute_target("sse4.2")
static pg_crc32c
pg_comp_crc32c_sse42_tail(pg_crc32c crc, const void *data, size_t len)
{
	const unsigned char *p = data;
	const unsigned char *pend = p + len;

	/*
	 * Process eight bytes of data at a time.
	 *
	 * NB: We do unaligned accesses here. The Intel architecture allows that,
	 * and performance testing didn't show any performance gain from aligning
	 * the begin address.
	 */
#ifdef __x86_64__
	while (p + 8 <= pend)
	{
		crc = (uint32) _mm_crc32_u64(crc, *((const uint64 *) p));
		p += 8;
	}

	/* Process remaining full four bytes if any */
	if (p + 4 <= pend)
	{
		crc = _mm_crc32_u32(crc, *((const unsigned int *) p));
		p += 4;
	}
#else

	/*
	 * Process four bytes at a time. (The eight byte instruction is not
	 * available on the 32-bit x86 architecture).
	 */
	while (p + 4 <= pend)
	{
		crc = _mm_crc32_u32(crc, *((const unsigned int *) p));
		p += 4;
	}
#endif							/* __x86_64__ */

	/* Process any remaining bytes one at a time. */
	while (p < pend)
	{
		crc = _mm_crc32_u8(crc, *p);
		p++;
	}

	return crc;
}

/* Generated by https://github.com/corsix/fast-crc32/ using: */
/* ./generate -i sse -p crc32c -a v4 */
/* MIT licensed */

#define clmul_lo(a, b) (_mm_clmulepi64_si128((a), (b), 0))
#define clmul_hi(a, b) (_mm_clmulepi64_si128((a), (b), 17))

pg_attribute_target("sse4.2,pclmul")
pg_crc32c
pg_comp_crc32c_sse42(pg_crc32c crc, const void *data, size_t length)
{
	/* adjust names to match generated code */
	pg_crc32c	crc0 = crc;
	size_t		len = length;
	const unsigned char *buf = data;

	if (len >= 128)
	{
		/* First vector chunk. */
		__m128i		x0 = _mm_loadu_si128((const __m128i *) buf),
					y0;
		__m128i		x1 = _mm_loadu_si128((const __m128i *) (buf + 16)),
					y1;
		__m128i		x2 = _mm_loadu_si128((const __m128i *) (buf + 32)),
					y2;
		__m128i		x3 = _mm_loadu_si128((const __m128i *) (buf + 48)),
					y3;
		__m128i		k;

		k = _mm_setr_epi32(0x740eef02, 0, 0x9e4addf8, 0);
		x0 = _mm_xor_si128(_mm_cvtsi32_si128(crc0), x0);
		buf += 64;
		len -= 64;

		/* Main loop. */
		while (len >= 64)
		{
			y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
			y1 = clmul_lo(x1, k), x1 = clmul_hi(x1, k);
			y2 = clmul_lo(x2, k), x2 = clmul_hi(x2, k);
			y3 = clmul_lo(x3, k), x3 = clmul_hi(x3, k);
			y0 = _mm_xor_si128(y0, _mm_loadu_si128((const __m128i *) buf)), x0 = _mm_xor_si128(x0, y0);
			y1 = _mm_xor_si128(y1, _mm_loadu_si128((const __m128i *) (buf + 16))), x1 = _mm_xor_si128(x1, y1);
			y2 = _mm_xor_si128(y2, _mm_loadu_si128((const __m128i *) (buf + 32))), x2 = _mm_xor_si128(x2, y2);
			y3 = _mm_xor_si128(y3, _mm_loadu_si128((const __m128i *) (buf + 48))), x3 = _mm_xor_si128(x3, y3);
			buf += 64;
			len -= 64;
		}

		/* Reduce x0 ... x3 to just x0. */
		k = _mm_setr_epi32(0xf20c0dfe, 0, 0x493c7d27, 0);
		y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
		y2 = clmul_lo(x2, k), x2 = clmul_hi(x2, k);
		y0 = _mm_xor_si128(y0, x1), x0 = _mm_xor_si128(x0, y0);
		y2 = _mm_xor_si128(y2, x3), x2 = _mm_xor_si128(x2, y2);
		k = _mm_setr_epi32(0x3da6d0cb, 0, 0xba4fc28e, 0);
		y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
		y0 = _mm_xor_si128(y0, x2), x0 = _mm_xor_si128(x0, y0);

		/* Reduce 128 bits to 32 bits, and multiply by x^32. */
		crc0 = _mm_crc32_u64(0, _mm_extract_epi64(x0, 0));
		crc0 = _mm_crc32_u64(crc0, _mm_extract_epi64(x0, 1));
	}

	return pg_comp_crc32c_sse42_tail(crc0, buf, len);
}
